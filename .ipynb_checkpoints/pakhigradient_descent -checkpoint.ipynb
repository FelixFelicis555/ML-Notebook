{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def step_gradient(points, learning_rate , m ):\n",
    "    m_slope=np.zeros(points.shape[1])\n",
    "    new_m=np.zeros(points.shape[1])\n",
    "    M= points.shape[0] \n",
    "    N= points.shape[1]\n",
    "    x_unscaled = points[:,0:N-1]\n",
    "    y_train=points[:,N-1]\n",
    "\n",
    "    x_scaled= preprocessing.scale(x_unscaled)\n",
    " \n",
    "    \n",
    "    \n",
    "    for j in range(N):\n",
    "        for i in range(M) :\n",
    "            y= y_train[i] #y_train value for each row\n",
    "            x=x_scaled[i]       #1st row\n",
    "            x=np.append(x,1)         #appendind 1 in the end of the row\n",
    "            #print(\"x \",x, \"y\", y )\n",
    "            #print(\"x.shape\",x.shape , \"y shape \",y.shape ,\"m shap\",m.shape)\n",
    "            mul=(m*x).sum()   #m1x1* m2*x2.....\n",
    "            m_slope[j] += (-2/M)* (y - mul) *x[j]\n",
    "            \n",
    "        new_m[j] = m[j] -learning_rate*m_slope[j]\n",
    "    \n",
    "    \n",
    "    return new_m \n",
    "    #return new_m , new_c\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gd(points,learning_rate,num_iterations):\n",
    "    m=np.zeros(points.shape[1])\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        m= step_gradient(points, learning_rate , m )\n",
    "        print(i, \"cost \", cost(points,m ))\n",
    "    return m \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cost(points, m ):\n",
    "    total_cost =0\n",
    "    M= points.shape[0] \n",
    "    N= points.shape[1]\n",
    "    for i in range(M) :\n",
    "            y=points[i][N-1]\n",
    "            x=points[i]\n",
    "            x[N-1]=1\n",
    "            mul=(m*x).sum()\n",
    "            total_cost += (1/M)*((y - mul)**2)\n",
    "            \n",
    "       \n",
    "    return total_cost\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(data_test,m):\n",
    "    M=data_test.shape[0]\n",
    "    N=data_test.shape[1]\n",
    "    y_pred=np.zeros(M)\n",
    "    #c=m[N]\n",
    "    #m=m[0:N]\n",
    "    for i in range(M):\n",
    "        x_test=data_test[i]\n",
    "        \n",
    "        x_test=np.append(x_test,1)\n",
    "        \n",
    "        #print(\"x\",x, \"m\" , m)\n",
    "        y_pred[i]= (x_test*m).sum()\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    data =np.genfromtxt(\"training_boston_x_y_train.csv\",delimiter =\",\")\n",
    "    learning_rate=0.1\n",
    "    num_iterations=100\n",
    "    m =gd(data,learning_rate,num_iterations)\n",
    "    print(m)\n",
    "    data_test=np.genfromtxt(\"test_boston_x_test.csv\",delimiter =\",\")\n",
    "    y_pred=predict(data_test,m)\n",
    "    print(y_pred)\n",
    "    np.savetxt('pred_gradient_descent.csv',y_pred ,delimiter=\",\",fmt=\"%.5f\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 cost  378.6558093126514\n",
      "1 cost  11.501857315641862\n",
      "2 cost  5.818625905338149\n",
      "3 cost  3.6538383698427745\n",
      "4 cost  2.3033664396841096\n",
      "5 cost  1.461299954214471\n",
      "6 cost  0.9280233283359131\n",
      "7 cost  0.5905327293939981\n",
      "8 cost  0.3764116085333726\n",
      "9 cost  0.2403861703434319\n",
      "10 cost  0.15385119214334098\n",
      "11 cost  0.09873156891830515\n",
      "12 cost  0.0635781915489311\n",
      "13 cost  0.04112879827708434\n",
      "14 cost  0.026770627812195462\n",
      "15 cost  0.017570567436123046\n",
      "16 cost  0.011661758389316618\n",
      "17 cost  0.007855006377321228\n",
      "18 cost  0.00539223429583501\n",
      "19 cost  0.003789862471108127\n",
      "20 cost  0.002739211168316379\n",
      "21 cost  0.0020431089445989968\n",
      "22 cost  0.0015755153512499558\n",
      "23 cost  0.0012557865701759378\n",
      "24 cost  0.0010322650281410634\n",
      "25 cost  0.0008718100562498362\n",
      "26 cost  0.000753116166649525\n",
      "27 cost  0.000662447185240869\n",
      "28 cost  0.0005909118857727369\n",
      "29 cost  0.000532723535949012\n",
      "30 cost  0.0004840876244880238\n",
      "31 cost  0.00044249074244703324\n",
      "32 cost  0.0004062456911506762\n",
      "33 cost  0.0003742002787789902\n",
      "34 cost  0.0003455507092324896\n",
      "35 cost  0.0003197218185533546\n",
      "36 cost  0.0002962900492101097\n",
      "37 cost  0.0002749337609126943\n",
      "38 cost  0.0002554010388836549\n",
      "39 cost  0.0002374887133966368\n",
      "40 cost  0.0002210285737850411\n",
      "41 cost  0.00020587820963755482\n",
      "42 cost  0.00019191483768458812\n",
      "43 cost  0.00017903106413405957\n",
      "44 cost  0.0001671319098053589\n",
      "45 cost  0.0001561326665538222\n",
      "46 cost  0.00014595730750108953\n",
      "47 cost  0.00013653727199516624\n",
      "48 cost  0.00012781050913739442\n",
      "49 cost  0.00011972070397718735\n",
      "50 cost  0.00011221663628781294\n",
      "51 cost  0.00010525163842875287\n",
      "52 cost  9.878312950847563e-05\n",
      "53 cost  9.277221001286966e-05\n",
      "54 cost  8.718330561458637e-05\n",
      "55 cost  8.198385189075415e-05\n",
      "56 cost  7.714401370095745e-05\n",
      "57 cost  7.263643436447263e-05\n",
      "58 cost  6.84360107487526e-05\n",
      "59 cost  6.451969108253951e-05\n",
      "60 cost  6.0866292827821056e-05\n",
      "61 cost  5.745633834354421e-05\n",
      "62 cost  5.4271906388228445e-05\n",
      "63 cost  5.129649776288739e-05\n",
      "64 cost  4.851491360607455e-05\n",
      "65 cost  4.5913145030176994e-05\n",
      "66 cost  4.3478272939643617e-05\n",
      "67 cost  4.119837700278517e-05\n",
      "68 cost  3.906245286282504e-05\n",
      "69 cost  3.706033677377574e-05\n",
      "70 cost  3.51826369345886e-05\n",
      "71 cost  3.342067087255331e-05\n",
      "72 cost  3.1766408295486187e-05\n",
      "73 cost  3.021241889297408e-05\n",
      "74 cost  2.8751824620821178e-05\n",
      "75 cost  2.7378256050683648e-05\n",
      "76 cost  2.608581240941218e-05\n",
      "77 cost  2.4869024970472167e-05\n",
      "78 cost  2.3722823493521608e-05\n",
      "79 cost  2.2642505438285702e-05\n",
      "80 cost  2.162370770568893e-05\n",
      "81 cost  2.0662380683169124e-05\n",
      "82 cost  1.9754764392524247e-05\n",
      "83 cost  1.889736655782616e-05\n",
      "84 cost  1.8086942428123756e-05\n",
      "85 cost  1.7320476205078283e-05\n",
      "86 cost  1.65951639395241e-05\n",
      "87 cost  1.5908397773399126e-05\n",
      "88 cost  1.5257751414695919e-05\n",
      "89 cost  1.4640966743185642e-05\n",
      "90 cost  1.4055941453774021e-05\n",
      "91 cost  1.3500717652577972e-05\n",
      "92 cost  1.2973471328245829e-05\n",
      "93 cost  1.2472502627776707e-05\n",
      "94 cost  1.1996226872189589e-05\n",
      "95 cost  1.1543166252921165e-05\n",
      "96 cost  1.1111942154850392e-05\n",
      "97 cost  1.0701268056405712e-05\n",
      "98 cost  1.0309942961357468e-05\n",
      "99 cost  9.936845320669929e-06\n",
      "[-7.53034704e-04  1.73772683e-03  2.70712139e-03 -2.59772648e-04\n",
      " -2.79256033e-03 -1.04545689e-03  5.68164358e-04 -2.13986660e-03\n",
      "  7.05702447e-03 -7.22836523e-03 -6.66852862e-04  3.08736532e-05\n",
      " -2.17910197e-04  1.00000000e+00]\n",
      "[1.00316008 1.00150247 0.99955724 1.00136688 0.99682781 0.99965332\n",
      " 1.00624937 1.00256379 0.99865468 1.00054905 0.99839843 0.99839028\n",
      " 0.99236493 0.99827213 0.99755015 0.99769416 0.99995471 0.99867627\n",
      " 0.99815014 0.99797242 0.99692532 1.00177871 1.00104572 1.00318617\n",
      " 0.99839936 1.00403417 1.00213988 0.99870436 1.00410641 1.00155973\n",
      " 0.99793002 0.9981938  0.9984836  1.00014528 1.0000722  1.00091911\n",
      " 1.00061818 0.99762293 1.00200215 0.99961821 1.00424806 1.00214586\n",
      " 0.99963111 0.99757557 1.00019206 1.00227686 1.00000632 0.99953561\n",
      " 1.00318356 0.99904305 1.00096896 1.00707489 0.99906962 0.99788841\n",
      " 0.99809047 1.000869   0.99976067 1.00018532 0.99830376 0.99993476\n",
      " 0.99822912 0.9997614  0.99916885 0.9991757  1.00149574 1.00273819\n",
      " 1.00082582 0.99974973 1.0033489  1.00089982 0.99837351 1.0014066\n",
      " 0.99629002 1.00173505 1.00501417 0.99958703 1.00442723 1.00271265\n",
      " 1.00094702 0.99909739 0.99867419 0.99764695 0.99214391 1.00200879\n",
      " 0.99908252 0.99741757 1.00097516 1.00207355 1.01007643 1.00091098\n",
      " 1.00053767 0.9977372  0.99753738 0.99953557 0.99761451 0.99533687\n",
      " 1.00313003 1.01031715 1.00293167 1.00336265 1.00297099 0.99961912\n",
      " 1.00338778 0.99931468 0.99902421 1.00124718 0.99730695 0.9978263\n",
      " 1.00067482 0.99945399 0.99839353 1.00382939 0.99802926 1.0025974\n",
      " 1.00010733 0.99715812 0.99710445 1.00107178 0.99711001 0.99696762\n",
      " 0.99855448 1.00470387 0.99908    1.00306876 0.99188038 0.99818078\n",
      " 0.99663938]\n"
     ]
    }
   ],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
